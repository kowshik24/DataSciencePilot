{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installing libraries"
      ],
      "metadata": {
        "id": "iGE0QoMgupVl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-l9NsXNInYKM"
      },
      "outputs": [],
      "source": [
        "!pip install langchain==0.0.225"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ctransformers==0.2.5"
      ],
      "metadata": {
        "id": "puRLCaEnndWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers==2.2.2"
      ],
      "metadata": {
        "id": "Th-E9CJ-ng1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pinecone-client==2.2.4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "id": "grX93UPB0zE0",
        "outputId": "24fb20a8-f1dc-4e7d-a44a-9a4030830a01"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pinecone-client==2.2.4\n",
            "  Downloading pinecone_client-2.2.4-py3-none-any.whl (179 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.4/179.4 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client==2.2.4) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.4 in /usr/local/lib/python3.10/dist-packages (from pinecone-client==2.2.4) (6.0.1)\n",
            "Collecting loguru>=0.5.0 (from pinecone-client==2.2.4)\n",
            "  Downloading loguru-0.7.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from pinecone-client==2.2.4) (4.5.0)\n",
            "Collecting dnspython>=2.0.0 (from pinecone-client==2.2.4)\n",
            "  Downloading dnspython-2.4.2-py3-none-any.whl (300 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.4/300.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from pinecone-client==2.2.4) (2.8.2)\n",
            "Requirement already satisfied: urllib3>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from pinecone-client==2.2.4) (2.0.7)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from pinecone-client==2.2.4) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client==2.2.4) (1.23.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.5.3->pinecone-client==2.2.4) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pinecone-client==2.2.4) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pinecone-client==2.2.4) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pinecone-client==2.2.4) (2023.11.17)\n",
            "Installing collected packages: loguru, dnspython, pinecone-client\n",
            "Successfully installed dnspython-2.4.2 loguru-0.7.2 pinecone-client-2.2.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pinecone"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf"
      ],
      "metadata": {
        "id": "G5Z6-8l-npNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing all the modules"
      ],
      "metadata": {
        "id": "me_vFWtPutIh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Pinecone\n",
        "import pinecone\n",
        "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.llms import CTransformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Il8Q1X0xnrxe",
        "outputId": "4ab5a62e-f450-4a24-82c8-49883325e734"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pinecone/index.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download the model from huggingface"
      ],
      "metadata": {
        "id": "V5jeqHvlsvzY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML/raw/main/llama-2-7b-chat.ggmlv3.q4_0.bin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjeSBBG4syyv",
        "outputId": "327b158e-49d2-43bb-ba27-962fde57be5f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-01-17 18:39:36--  https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML/raw/main/llama-2-7b-chat.ggmlv3.q4_0.bin\n",
            "Resolving huggingface.co (huggingface.co)... 13.35.166.69, 13.35.166.50, 13.35.166.36, ...\n",
            "Connecting to huggingface.co (huggingface.co)|13.35.166.69|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 135 [text/plain]\n",
            "Saving to: ‘llama-2-7b-chat.ggmlv3.q4_0.bin’\n",
            "\n",
            "\r          llama-2-7   0%[                    ]       0  --.-KB/s               \rllama-2-7b-chat.ggm 100%[===================>]     135  --.-KB/s    in 0s      \n",
            "\n",
            "2024-01-17 18:39:36 (133 MB/s) - ‘llama-2-7b-chat.ggmlv3.q4_0.bin’ saved [135/135]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unzip the data files"
      ],
      "metadata": {
        "id": "dOTGd0OJukhK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/test_data.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InRND0tZrZG_",
        "outputId": "01899ac0-a500-458c-d92a-4e70e294f1e6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/test_data.zip\n",
            "  inflating: test_data/2-Aurelien-Geron-Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-Tensorflow_-Concepts-Tools-and-Techniques-to-Build-Intelligent-Systems-OReilly-Media-2019.pdf  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up the API KEYS"
      ],
      "metadata": {
        "id": "gtLUHRW2uwf_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PINECONE_API_KEY = \"a36608c4-ec9b-4c79-804d-ab3684ead68c\"\n",
        "PINECONE_API_ENV = \"gcp-starter\""
      ],
      "metadata": {
        "id": "dg99UVsLnxDP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZpvEmA198Grs",
        "outputId": "47c781ae-8629-4684-f56b-dd9c3b540052"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_pdf(data):\n",
        "    pdf_loader_kwargs={'autodetect_encoding': True}\n",
        "    loader = DirectoryLoader(data,\n",
        "                    glob=\"*.pdf\",\n",
        "            loader_cls=PyPDFLoader,show_progress=True)\n",
        "\n",
        "    documents = loader.load()\n",
        "\n",
        "    return documents"
      ],
      "metadata": {
        "id": "6T8I-I0MonbG"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extracted_data = load_pdf(\"/content/test_data/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvD3wASssBap",
        "outputId": "f429612d-fe69-4b4c-b1d4-4ac567b2de47"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:07<00:00,  7.37s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(extracted_data[100].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rIrlsOdxwOOx",
        "outputId": "3fd1886b-784c-4a46-99c2-a5c414806964"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2203"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "extracted_data[100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AqJM53yh-TqM",
        "outputId": "30807af7-13a6-4e12-de84-dbb53a18f6f0"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='Instead of a transformer, you can specify the string \"drop\"  if you\\nwant the columns to be dropped. Or you can specify \"pass\\nthrough\"  if you want the columns to be left untouched. By default,\\nthe remaining columns (i.e., the ones that were not listed) will be\\ndropped, but you can set the remainder  hyperparameter to any\\ntransformer (or to \"passthrough\" ) if you want these columns to be\\nhandled differently.\\nIf you are using Scikit-Learn 0.19 or earlier, you can use a third-party library such as\\nsklearn-pandas , or roll out your own custom transformer to get the same function‐\\nality as the ColumnTransformer . Alternatively, you can use the FeatureUnion  class\\nwhich can also apply different transformers and concatenate their outputs, but you\\ncannot specify different columns for each transformer, they all apply to the whole\\ndata. It is possible to work around this limitation using a custom transformer for col‐\\numn selection (see the Jupyter notebook for an example).\\nSelect and Train a Model\\nAt last! Y ou framed the problem, you got the data and explored it, you sampled a\\ntraining set and a test set, and you wrote transformation pipelines to clean up and\\nprepare your data for Machine Learning algorithms automatically. Y ou are now ready\\nto select and train a Machine Learning model.\\nTraining and Evaluating on the Training Set\\nThe good news is that thanks to all these previous steps, things are now going to be\\nmuch simpler than you might think. Let’s first train a Linear Regression model, like\\nwe did in the previous chapter:\\nfrom sklearn.linear_model  import LinearRegression\\nlin_reg = LinearRegression ()\\nlin_reg.fit(housing_prepared , housing_labels )\\nDone! Y ou now have a working Linear Regression model. Let’s try it out on a few\\ninstances from the training set:\\n>>> some_data  = housing.iloc[:5]\\n>>> some_labels  = housing_labels .iloc[:5]\\n>>> some_data_prepared  = full_pipeline .transform (some_data )\\n>>> print(\"Predictions:\" , lin_reg.predict(some_data_prepared ))\\nPredictions: [ 210644.6045  317768.8069  210956.4333  59218.9888  189747.5584]\\n>>> print(\"Labels:\" , list(some_labels ))\\nLabels: [286600.0, 340600.0, 196900.0, 46300.0, 254500.0]\\nSelect and Train a Model | 75', metadata={'source': '/content/test_data/2-Aurelien-Geron-Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-Tensorflow_-Concepts-Tools-and-Techniques-to-Build-Intelligent-Systems-OReilly-Media-2019.pdf', 'page': 100})"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_split(extracted_data):\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 20)\n",
        "    text_chunks = text_splitter.split_documents(extracted_data)\n",
        "\n",
        "    return text_chunks"
      ],
      "metadata": {
        "id": "zQh1vhSWsDTm"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_chunks = text_split(extracted_data)\n",
        "print(\"length of my chunk:\", len(text_chunks))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vR70Tg1IsFmZ",
        "outputId": "0b9f5f22-eacc-4604-db8f-a24457a00998"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of my chunk: 2335\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def download_hugging_face_embeddings():\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "    return embeddings"
      ],
      "metadata": {
        "id": "UaQ1pFQhsH-3"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = download_hugging_face_embeddings()"
      ],
      "metadata": {
        "id": "hznZyEsqsJjh"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_result = embeddings.embed_query(\"Hello world\")\n",
        "print(\"Length\", len(query_result))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQnLay5TsNPH",
        "outputId": "8d4c5a75-e79c-40f5-b6b5-8bbe42657145"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length 384\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(text_chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lmyf_nIU2n0g",
        "outputId": "69064a59-d5ec-4bcc-eebc-fe77791ca464"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2335"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Initializing the Pinecone\n",
        "import pinecone\n",
        "pinecone.init(api_key=PINECONE_API_KEY,\n",
        "              environment=PINECONE_API_ENV)\n",
        "\n",
        "index_name=\"datascience-pilot\""
      ],
      "metadata": {
        "id": "in-HwEDw60Qh"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iv5UHnnA66mR",
        "outputId": "f8d1df3c-d047-4a9b-dd1f-9025d80bb970"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "HuggingFaceEmbeddings(client=SentenceTransformer(\n",
              "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
              "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
              "  (2): Normalize()\n",
              "), model_name='sentence-transformers/all-MiniLM-L6-v2', cache_folder=None, model_kwargs={}, encode_kwargs={})"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_malformed_surrogates(text, replacement=' '):\n",
        "    \"\"\"\n",
        "    Clean malformed surrogate pairs in the text.\n",
        "    :param text: The input text string.\n",
        "    :param replacement: The string to replace malformed surrogates with.\n",
        "    :return: Cleaned text string.\n",
        "    \"\"\"\n",
        "    new_text = []\n",
        "    skip_char = False\n",
        "    for i, char in enumerate(text):\n",
        "        if skip_char:\n",
        "            skip_char = False\n",
        "            continue\n",
        "\n",
        "        if 0xD800 <= ord(char) <= 0xDBFF:  # High surrogate\n",
        "            if i+1 < len(text) and 0xDC00 <= ord(text[i+1]) <= 0xDFFF:  # Correct low surrogate\n",
        "                new_text.append(char + text[i+1])\n",
        "                skip_char = True\n",
        "            else:  # Malformed surrogate pair\n",
        "                new_text.append(replacement)\n",
        "        else:\n",
        "            new_text.append(char)\n",
        "    return ''.join(new_text)\n",
        "\n",
        "# Use this function on your text data\n",
        "cleaned_text_chunks = [clean_malformed_surrogates(t.page_content) for t in text_chunks]\n",
        "\n",
        "# Then proceed with your embedding and Pinecone processes\n",
        "docsearch = Pinecone.from_texts(cleaned_text_chunks, embeddings, index_name=index_name)\n"
      ],
      "metadata": {
        "id": "LaY4ijSisWB4"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#If we already have an index we can load it like this\n",
        "docsearch=Pinecone.from_existing_index(index_name, embeddings)\n",
        "\n",
        "query = \"What is Nonsaturating Activation Functions?\"\n",
        "\n",
        "docs=docsearch.similarity_search(query, k=3)\n",
        "\n",
        "print(\"Result\", docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdJBn4VTsZfM",
        "outputId": "7b72c2fc-6c4e-4921-bfcf-213e6060dec1"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result [Document(page_content='These popular activation functions and their derivatives are represented in\\nFigure 10-8 . But wait! Why do we need activation functions in the first place? Well, if\\nyou chain several linear transformations, all you get is a linear transformation. For\\nexample, say f( x) = 2 x + 3 and g( x) = 5 x - 1, then chaining these two linear functions\\ngives you another linear function: f(g( x)) = 2(5 x - 1) + 3 = 10 x + 1. So if you don’t', metadata={}), Document(page_content='7“Self-Normalizing Neural Networks, \" G. Klambauer, T. Unterthiner and A. Mayr (2017).\\nFigure 11-3. ELU activation function\\nIt looks a lot like the ReLU function, with a few major differences:\\n•First it takes on negative values when z < 0, which allows the unit to have an\\naverage output closer to 0. This helps alleviate the vanishing gradients problem,\\nas discussed earlier. The hyperparameter α defines the value that the ELU func‐', metadata={}), Document(page_content='tor of bias terms (one per neuron). When it receives some input data, it computes\\nEquation 10-2 .\\n•Next we add a second Dense  hidden layer with 100 neurons, also using the ReLU\\nactivation function.\\n•Finally, we add a Dense  output layer with 10 neurons (one per class), using the\\nsoftmax activation function (because the classes are exclusive).\\nSpecifying activation=\"relu\"  is equivalent to activa\\ntion=keras.activations.relu . Other activation functions are', metadata={})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template=\"\"\"\n",
        "Use the following pieces of information to answer the user's question.\n",
        "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "\n",
        "Context: {context}\n",
        "Question: {question}\n",
        "\n",
        "Only return the helpful answer below and nothing else.\n",
        "Helpful answer:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "BIp0uArksbNS"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PROMPT=PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
        "chain_type_kwargs={\"prompt\": PROMPT}"
      ],
      "metadata": {
        "id": "r4LIGJ9esdTL"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install git-lfs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bGAtt6XeFYWw",
        "outputId": "cdd7a735-3c45-4b17-8288-8082603bbc3b"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "git-lfs is already the newest version (3.0.2-1ubuntu0.2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 24 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone only the required file to save space and time\n",
        "!git lfs clone --depth 1 https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkzCVjKaFeCd",
        "outputId": "aec51ce2-cd3b-43ef-fe15-c1414c4c215a"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: 'git lfs clone' is deprecated and will not be updated\n",
            "          with new flags from 'git clone'\n",
            "\n",
            "'git clone' has been updated in upstream Git to have comparable\n",
            "speeds to 'git lfs clone'.\n",
            "Cloning into 'Llama-2-7B-Chat-GGML'...\n",
            "remote: Enumerating objects: 22, done.\u001b[K\n",
            "remote: Counting objects: 100% (22/22), done.\u001b[K\n",
            "remote: Compressing objects: 100% (21/21), done.\u001b[K\n",
            "remote: Total 22 (delta 0), reused 22 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (22/22), 16.28 KiB | 2.03 MiB/s, done.\n",
            "Downloading LFS objects:  43% (6/14), 42 GB | 76 MB/s\n",
            "Exiting because of \"interrupt\" signal.\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd Llama-2-7B-Chat-GGML"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOfrNGemFjeI",
        "outputId": "6d12ebdd-e50b-4f76-fbf9-da64cd850cbf"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Llama-2-7B-Chat-GGML\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git lfs pull --include=\"llama-2-7b-chat.ggmlv3.q4_0.bin\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7njk4n-kFrBY",
        "outputId": "66c1cec0-dccf-4b5c-e453-c76a5f8bc9d0"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Exiting because of \"interrupt\" signal.\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm=CTransformers(model=\"/content/Llama-2-7B-Chat-GGML/llama-2-7b-chat.ggmlv3.q4_0.bin\",\n",
        "                  model_type=\"llama\",\n",
        "                  config={'max_new_tokens':512,\n",
        "                          'temperature':0.8})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "TQ35u4DMse9u",
        "outputId": "d99767dc-f311-446c-a2d6-c23e3b4f353a"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Failed to create LLM 'llama' from '/content/Llama-2-7B-Chat-GGML/llama-2-7b-chat.ggmlv3.q4_0.bin'.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-93-48a576b166fb>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m llm=CTransformers(model=\"/content/Llama-2-7B-Chat-GGML/llama-2-7b-chat.ggmlv3.q4_0.bin\",\n\u001b[0m\u001b[1;32m      2\u001b[0m                   \u001b[0mmodel_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"llama\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                   config={'max_new_tokens':512,\n\u001b[1;32m      4\u001b[0m                           'temperature':0.8})\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/load/serializable.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lc_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pydantic/main.cpython-310-x86_64-linux-gnu.so\u001b[0m in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pydantic/main.cpython-310-x86_64-linux-gnu.so\u001b[0m in \u001b[0;36mpydantic.main.validate_model\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/llms/ctransformers.py\u001b[0m in \u001b[0;36mvalidate_environment\u001b[0;34m(cls, values)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"config\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         values[\"client\"] = AutoModelForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m     71\u001b[0m             \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0mmodel_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_type\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ctransformers/hub.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, model_path_or_repo_id, model_type, model_file, config, lib, local_files_only, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m             )\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         return LLM(\n\u001b[0m\u001b[1;32m    158\u001b[0m             \u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0mmodel_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ctransformers/llm.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_path, model_type, config, lib)\u001b[0m\n\u001b[1;32m    203\u001b[0m         )\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_llm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m    206\u001b[0m                 \u001b[0;34mf\"Failed to create LLM '{model_type}' from '{model_path}'.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             )\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Failed to create LLM 'llama' from '/content/Llama-2-7B-Chat-GGML/llama-2-7b-chat.ggmlv3.q4_0.bin'."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "qa=RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=docsearch.as_retriever(search_kwargs={'k': 2}),\n",
        "    return_source_documents=True,\n",
        "    chain_type_kwargs=chain_type_kwargs)"
      ],
      "metadata": {
        "id": "Re39DRNRwluW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    user_input=input(f\"Input Prompt:\")\n",
        "    result=qa({\"query\": user_input})\n",
        "    print(\"Response : \", result[\"result\"])"
      ],
      "metadata": {
        "id": "bd32VU1bwoG4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}